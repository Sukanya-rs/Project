{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Feed_Forward_Neural_Networks","provenance":[{"file_id":"https://github.com/chokkan/deeplearningclass/blob/master/mlp_binary.ipynb","timestamp":1654588551988}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"CX_9BuPB_hA-"},"cell_type":"markdown","source":["# Feedforward Neural Networks\n","\n","This Jupyter notebook explains various ways of implementing single-layer and multi-layer neural networks. The implementations are arranged by concrete (explicit) to abstract order so that one can understand the black-boxed processing in deep learning frameworks.\n","\n","In order to focus on understanding the internals of training, this notebook uses a simple and classic example: *threshold logic units*.\n","Supposing $x=0$ as *false* and $x=1$ as *true*, single-layer neural networks can realize logic units such as AND ($\\wedge$), OR ($\\vee$), NOT ($\\lnot$), and NAND ($|$). Multi-layer neural networks can realize logical compounds such as XOR.\n","\n","| $x_1$ | $x_2$ | AND | OR | NAND | XOR |\n","| :---: |:-----:|:---:|:--:|:----:|:---:|\n","| 0 | 0 | 0 | 0 | 1 | 0 |\n","| 0 | 1 | 0 | 1 | 1 | 1 |\n","| 1 | 0 | 0 | 1 | 1 | 1 |\n","| 1 | 1 | 1 | 1 | 0 | 0 |\n"]},{"metadata":{"id":"qxdFG8U2Net8"},"cell_type":"markdown","source":["## Using numpy"]},{"metadata":{"id":"Ea5cM4JEsENr"},"cell_type":"markdown","source":["### Single-layer perceptron\n","\n","A single layer perceptron predicts a binary label $\\hat{y} \\in \\{0, 1\\}$ for a given input vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ ($d$ presents the number of dimensions of inputs) by using the following formula,\n","$$\n","\\hat{y} = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = g(w_1 x_1 + w_2 x_2 + ... + w_d x_d + b)\n","$$\n","\n","Here, $\\boldsymbol{w} \\in \\mathbb{R}^d$ is a weight vector; $b \\in \\mathbb{R}$ is a bias weight; and $g(.)$ denotes a Heaviside step function (we assume $g(0)=0$).\n","\n","Let's train a NAND gate with two inputs ($d = 2$). More specifically, we want to find a weight vector $\\boldsymbol{w}$ and a bias weight $b$ of a single-layer perceptron that realizes the truth table of the NAND gate: $\\{0,1\\}^2 \\to \\{0,1\\}$.\n","\n","| $x_1$ | $x_2$ | $y$  |\n","| :---: |:-----:|:----:|\n","| 0 | 0 | 1|\n","| 0 | 1 | 1|\n","| 1 | 0 | 1|\n","| 1 | 1 | 0|\n","\n","We convert the truth table into a training set consisting of all mappings of the NAND gate,\n","$$\n","\\boldsymbol{x}_1 = (0, 0), y_1 = 1 \\\\\n","\\boldsymbol{x}_2 = (0, 1), y_2 = 1 \\\\\n","\\boldsymbol{x}_3 = (1, 0), y_3 = 1 \\\\\n","\\boldsymbol{x}_4 = (1, 1), y_4 = 0 \\\\\n","$$\n","\n","In order to train a weight vector and bias weight in a unified code, we include a bias term as an additional dimension to inputs. More concretely, we append $1$ to each input,\n","$$\n","\\boldsymbol{x}'_1 = (0, 0, 1), y_1 = 1 \\\\\n","\\boldsymbol{x}'_2 = (0, 1, 1), y_2 = 1 \\\\\n","\\boldsymbol{x}'_3 = (1, 0, 1), y_3 = 1 \\\\\n","\\boldsymbol{x}'_4 = (1, 1, 1), y_4 = 0 \\\\\n","$$\n","\n","Then, the formula of the single-layer perceptron becomes,\n","$$\n","\\hat{y} = g((w_1, w_2, w_3) \\cdot \\boldsymbol{x}') = g(w_1 x_1 + w_2 x_2 + w_3)\n","$$\n","In other words, $w_1$ and $w_2$ present weights for $x_1$ and $x_2$, respectively, and $w_3$ does a bias weight.\n","\n","The code below implements Rosenblatt's perceptron algorithm with a fixed number of iterations (100 times). We use a constant learning rate 0.5 for simplicity.\n"]},{"metadata":{"id":"2ygoUjQYrPoj"},"cell_type":"code","source":["import numpy as np\n","\n","# Training data for NAND.\n","x = np.array([\n","    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n","    ])\n","y = np.array([0, 0, 0, 1])\n","w = np.array([0.0, 0.0, 0.0])\n","\n","eta = 0.5\n","for t in range(100):\n","    for i in range(len(y)):\n","        y_pred = np.heaviside(np.dot(x[i], w), 0)\n","        w += (y[i] - y_pred) * eta * x[i]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"TYoeshu2rXdK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c46715b-b140-47d0-9ef9-6fe85fc450b2","executionInfo":{"status":"ok","timestamp":1654588411837,"user_tz":-330,"elapsed":10,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["w"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1. ,  0.5, -1. ])"]},"metadata":{},"execution_count":2}]},{"metadata":{"id":"hOFgUFojraFA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"95f47667-32bb-4003-b21f-908e62873144","executionInfo":{"status":"ok","timestamp":1654588412466,"user_tz":-330,"elapsed":634,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["np.heaviside(np.dot(x, w), 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 1.])"]},"metadata":{},"execution_count":3}]},{"metadata":{"id":"bl_ZAEguNzgU"},"cell_type":"markdown","source":["### Single-layer perceptron with mini-batch\n","\n","It is desireable to reduce the execusion run by the Python interpreter, which is relatively slow. The common technique to speed up a machine-learning code written in Python is to to execute computations within the matrix library (e.g., numpy).\n","\n","The single-layer perceptron makes predictions for four inputs,\n","$$\n","\\hat{y}_1 = g(\\boldsymbol{x}_1 \\cdot \\boldsymbol{w}) \\\\\n","\\hat{y}_2 = g(\\boldsymbol{x}_2 \\cdot \\boldsymbol{w}) \\\\\n","\\hat{y}_3 = g(\\boldsymbol{x}_3 \\cdot \\boldsymbol{w}) \\\\\n","\\hat{y}_4 = g(\\boldsymbol{x}_4 \\cdot \\boldsymbol{w}) \\\\\n","$$\n","\n","Here, we define $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ and $X \\in \\mathbb{R}^{4 \\times d}$ as,\n","$$\n","\\hat{Y} = \\begin{pmatrix} \n","  \\hat{y}_1 \\\\ \n","  \\hat{y}_2 \\\\ \n","  \\hat{y}_3 \\\\ \n","  \\hat{y}_4 \\\\ \n","\\end{pmatrix},\n","X = \\begin{pmatrix} \n","  \\boldsymbol{x}_1 \\\\ \n","  \\boldsymbol{x}_2 \\\\ \n","  \\boldsymbol{x}_3 \\\\ \n","  \\boldsymbol{x}_4 \\\\ \n","\\end{pmatrix}\n","$$\n","\n","Then, we can write the four predictions in one dot-product computation,\n","$$\n","\\hat{Y} = X \\cdot \\boldsymbol{w}\n","$$\n","\n","The code below implements this idea. The function `np.heaviside()` yields a vector corresponding to the four predictions, applying the step function for every element of the argument.\n","\n","This technique is frequently used in mini-batch training."]},{"metadata":{"id":"2fK-_WimtPwb"},"cell_type":"code","source":["import numpy as np\n","\n","# Training data for NAND.\n","x = np.array([\n","    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n","    ])\n","y = np.array([1, 1, 1, 0])\n","w = np.array([0.0, 0.0, 0.0])\n","\n","eta = 0.5\n","for t in range(100):\n","    y_pred = np.heaviside(np.dot(x, w), 0)\n","    w += np.dot((y - y_pred), x)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"_x4p1BldtQ-K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"34d77335-b0bb-4fc7-c25f-ba4ed8771538","executionInfo":{"status":"ok","timestamp":1654588412467,"user_tz":-330,"elapsed":39,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["w"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1., -1.,  2.])"]},"metadata":{},"execution_count":5}]},{"metadata":{"id":"E-P2RpWrtVyf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a73da4a-b19e-4227-9d7d-bce5cef87433","executionInfo":{"status":"ok","timestamp":1654588412468,"user_tz":-330,"elapsed":32,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["np.heaviside(np.dot(x, w), 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1., 0.])"]},"metadata":{},"execution_count":6}]},{"metadata":{"id":"nkxBcCSTtDvm"},"cell_type":"markdown","source":["### Stochastic gradient descent (SGD) with mini-batch"]},{"metadata":{"id":"bltpfNRctjV5"},"cell_type":"code","source":["import numpy as np\n","\n","def sigmoid(v):\n","    return 1.0 / (1 + np.exp(-v))\n","\n","# Training data for NAND.\n","x = np.array([\n","    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n","    ])\n","y = np.array([1, 1, 1, 0])\n","w = np.array([0.0, 0.0, 0.0])\n","\n","eta = 0.5\n","for t in range(100):\n","    y_pred = sigmoid(np.dot(x, w))\n","    w -= np.dot((y_pred - y), x)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"9bASDMfhtm-I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"05b6d301-fbb6-4b11-9082-a94636b418c2","executionInfo":{"status":"ok","timestamp":1654588412469,"user_tz":-330,"elapsed":23,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["w"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-5.59504346, -5.59504346,  8.57206068])"]},"metadata":{},"execution_count":8}]},{"metadata":{"id":"-69r0c4KtqlW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aae8cef0-c00f-49ad-da29-1c8fb14c2806","executionInfo":{"status":"ok","timestamp":1654588412469,"user_tz":-330,"elapsed":16,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["sigmoid(np.dot(x, w))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.99981071, 0.95152498, 0.95152498, 0.06798725])"]},"metadata":{},"execution_count":9}]},{"metadata":{"id":"ELUeNFRFuJv3"},"cell_type":"markdown","source":["## Automatic differentiation"]},{"metadata":{"id":"aB0DwVOyuXP_"},"cell_type":"markdown","source":["### Using autograd\n","\n","Installing [autograd](https://github.com/HIPS/autograd) (do this once)."]},{"metadata":{"id":"kpt7DJ7a-qov","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"5d0d3f7b-8852-4dc3-dc74-1e9768595df3"},"cell_type":"code","source":["!pip install autograd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: autograd in /usr/local/lib/python3.6/dist-packages (1.2)\r\n","Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\r\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.5)\r\n"],"name":"stdout"}]},{"metadata":{"id":"DyC9iOFO-1cd","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"89aa3a6a-f865-477a-f4d9-c6f4f0a572f9"},"cell_type":"code","source":["import autograd\n","import autograd.numpy as np\n","\n","def loss(w, x):\n","    return -np.log(1.0 / (1 + np.exp(-np.dot(x, w))))\n","\n","x = np.array([1, 1, 1])\n","w = np.array([1.0, 1.0, -1.5])\n","\n","grad_loss = autograd.grad(loss)\n","print(loss(w, x))\n","print(grad_loss(w, x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.47407698418010663\n","[-0.37754067 -0.37754067 -0.37754067]\n"],"name":"stdout"}]},{"metadata":{"id":"59D2aARTuQDL"},"cell_type":"markdown","source":["### Using pytorch"]},{"metadata":{"id":"VOoZXvXP-6b2","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"d184e47f-ec66-4c78-e777-d928ac22e5a5"},"cell_type":"code","source":["!pip install torch torchvision"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.0)\r\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\r\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.2.0)\n"],"name":"stdout"}]},{"metadata":{"id":"nHixTUut_LIm","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"30d28589-4e9f-451c-ab36-d76db7887149"},"cell_type":"code","source":["import torch\n","\n","dtype = torch.float\n","\n","x = torch.tensor([1, 1, 1], dtype=dtype)\n","w = torch.tensor([1.0, 1.0, -1.5], dtype=dtype, requires_grad=True)\n","\n","loss = -torch.dot(x, w).sigmoid().log()\n","loss.backward()\n","print(loss.item())\n","print(w.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.4740769565105438\n","tensor([-0.3775, -0.3775, -0.3775])\n"],"name":"stdout"}]},{"metadata":{"id":"VFzuau5gu4vY"},"cell_type":"markdown","source":["## Implementing neural networks with pytorch"]},{"metadata":{"id":"STwWdvJCva4G"},"cell_type":"markdown","source":["### Single-layer neural network using automatic differentiation"]},{"metadata":{"id":"xOJHDGYKIgsm"},"cell_type":"code","source":["import torch\n","\n","dtype = torch.float\n","\n","# Training data for NAND.\n","x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n","y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n","w = torch.randn(3, 1, dtype=dtype, requires_grad=True)\n","\n","eta = 0.5\n","for t in range(100):\n","    # y_pred = \\sigma(x \\cdot w)\n","    y_pred = x.mm(w).sigmoid()\n","    ll = y * y_pred + (1 - y) * (1 - y_pred)\n","    loss = -ll.log().sum()      # The loss value.\n","    #print(t, loss.item())\n","    loss.backward()             # Compute the gradients of the loss.\n","\n","    with torch.no_grad():\n","        w -= eta * w.grad       # Update weights using SGD.        \n","        w.grad.zero_()          # Clear the gradients for the next iteration."],"execution_count":null,"outputs":[]},{"metadata":{"id":"FyoS5iP6n7Ru","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"fe6de935-5f28-4a67-9f97-b8447e138f2b"},"cell_type":"code","source":["w"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-4.2228],\n","        [-4.2234],\n","        [ 6.5268]])"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"Jfp604gFn9Yw","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"27fdd3f8-8607-4076-a962-0ac38618f8d8"},"cell_type":"code","source":["x.mm(w).sigmoid()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.9985],\n","        [ 0.9092],\n","        [ 0.9092],\n","        [ 0.1279]])"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"w6eu-AuDvl9J"},"cell_type":"markdown","source":["### Multi-layer neural network using automatic differentiation"]},{"metadata":{"id":"ts2RTKVPn_xk"},"cell_type":"code","source":["import torch\n","\n","dtype = torch.float\n","\n","# Training data for XOR.\n","x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n","y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n","w1 = torch.randn(3, 2, dtype=dtype, requires_grad=True)\n","w2 = torch.randn(2, 1, dtype=dtype, requires_grad=True)\n","b2 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n","\n","eta = 0.5\n","for t in range(1000):\n","    # y_pred = \\sigma(w_2 \\cdot \\sigma(x \\cdot w_1) + b_2)\n","    y_pred = x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()\n","    ll = y * y_pred + (1 - y) * (1 - y_pred)\n","    loss = -ll.log().sum()\n","    #print(t, loss.item())\n","    loss.backward()\n","    \n","    with torch.no_grad():\n","        # Update weights using SGD.\n","        w1 -= eta * w1.grad\n","        w2 -= eta * w2.grad\n","        b2 -= eta * b2.grad\n","        \n","        # Clear the gradients for the next iteration.\n","        w1.grad.zero_()\n","        w2.grad.zero_()\n","        b2.grad.zero_()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"FWgbqAXawEof","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"c1b87740-d7b4-4ba3-f327-81bf56199850"},"cell_type":"code","source":["print(w1)\n","print(w2)\n","print(b2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[-5.4129, -7.0510],\n","        [-5.4112, -7.0391],\n","        [ 8.0656,  2.9682]])\n","tensor([[ 11.6259],\n","        [-12.0312]])\n","tensor([[-5.4634]])\n"],"name":"stdout"}]},{"metadata":{"id":"yS4bql3foxB5","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"dadf8c7c-429e-426a-d9f4-53bf8e0fa4c6"},"cell_type":"code","source":["x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0050],\n","        [ 0.9945],\n","        [ 0.9945],\n","        [ 0.0084]])"]},"metadata":{"tags":[]},"execution_count":19}]},{"metadata":{"id":"OGS23bDSazMJ"},"cell_type":"markdown","source":["### Single-layer neural network with high-level NN modules"]},{"metadata":{"id":"Jt9eizLFo1iN"},"cell_type":"code","source":["import torch\n","\n","dtype = torch.float\n","\n","# Training data for NAND.\n","x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n","y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n","                                        \n","# Define a neural network using high-level modules.\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",")\n","\n","# Binary corss-entropy loss after sigmoid function.\n","loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n","\n","eta = 0.5\n","for t in range(100):\n","    y_pred = model(x)                   # Make predictions.\n","    loss = loss_fn(y_pred, y)           # Compute the loss.\n","    #print(t, loss.item())\n","    \n","    model.zero_grad()                   # Zero-clear the gradients.\n","    loss.backward()                     # Compute the gradients.\n","        \n","    with torch.no_grad():\n","        for param in model.parameters():\n","            param -= eta * param.grad   # Update the parameters using SGD."],"execution_count":null,"outputs":[]},{"metadata":{"id":"Zq6oqLmIENFa","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"83ddfa07-461e-4746-8669-6a74d6bdd3d8"},"cell_type":"code","source":["model.state_dict()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('0.weight', tensor([[-4.2632, -4.2634]])),\n","             ('0.bias', tensor([ 6.5866]))])"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"QdPOdNgO840b","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"afee61a6-608d-4ba0-9cac-e09947421f8c"},"cell_type":"code","source":["model(x).sigmoid()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.9986],\n","        [ 0.9108],\n","        [ 0.9108],\n","        [ 0.1256]])"]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"KfuoJMeqbClA"},"cell_type":"markdown","source":["### Multi-layer neural network with high-level NN modules"]},{"metadata":{"id":"D6ss25zA9nPk"},"cell_type":"code","source":["import torch\n","\n","dtype = torch.float\n","\n","# Training data for XOR.\n","x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n","y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n","                                        \n","# Define a neural network using high-level modules.\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n","    torch.nn.Sigmoid(),                 # Sigmoid function\n","    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",")\n","\n","# Binary corss-entropy loss after sigmoid function.\n","loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n","\n","eta = 0.5\n","for t in range(1000):\n","    y_pred = model(x)                   # Make predictions.\n","    loss = loss_fn(y_pred, y)           # Compute the loss.\n","    #print(t, loss.item())\n","    \n","    model.zero_grad()                   # Zero-clear the gradients.\n","    loss.backward()                     # Compute the gradients.\n","        \n","    with torch.no_grad():\n","        for param in model.parameters():\n","            param -= eta * param.grad   # Update the parameters using SGD."],"execution_count":null,"outputs":[]},{"metadata":{"id":"iLFGZWL0--2n","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"0f2c7a0f-c31e-486b-994b-8d91f20e0835"},"cell_type":"code","source":["model.state_dict()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('0.weight', tensor([[-5.3436, -5.3484],\n","                      [-6.9310, -6.9626]])),\n","             ('0.bias', tensor([ 7.9648,  2.9143])),\n","             ('2.weight', tensor([[ 11.4116, -11.8373]])),\n","             ('2.bias', tensor([-5.3505]))])"]},"metadata":{"tags":[]},"execution_count":24}]},{"metadata":{"id":"1BF6-W_J-82M","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"f6d54774-9eb7-49eb-a6cc-8fd3fc8cae4a"},"cell_type":"code","source":["model(x).sigmoid()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0056],\n","        [ 0.9938],\n","        [ 0.9938],\n","        [ 0.0095]])"]},"metadata":{"tags":[]},"execution_count":25}]},{"metadata":{"id":"gGK3DJBubb0f"},"cell_type":"markdown","source":["### Single-layer neural network with an optimizer."]},{"metadata":{"id":"puqbP4F9bidv"},"cell_type":"code","source":["import torch\n","\n","dtype = torch.float\n","\n","# Training data for NAND.\n","x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n","y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n","                                        \n","# Define a neural network using high-level modules.\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",")\n","\n","# Binary corss-entropy loss after sigmoid function.\n","loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n","\n","# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n","\n","for t in range(100):\n","    y_pred = model(x)           # Make predictions.\n","    loss = loss_fn(y_pred, y)   # Compute the loss.\n","    #print(t, loss.item())\n","    \n","    optimizer.zero_grad()       # Zero-clear gradients.\n","    loss.backward()             # Compute the gradients.\n","    optimizer.step()            # Update the parameters using the gradients."],"execution_count":null,"outputs":[]},{"metadata":{"id":"RBUX1BUhcDK4","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"99f6202e-af36-49b8-85e9-aca6d08d13b3"},"cell_type":"code","source":["model.state_dict()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('0.weight', tensor([[-4.2689, -4.2692]])),\n","             ('0.bias', tensor([ 6.5951]))])"]},"metadata":{"tags":[]},"execution_count":27}]},{"metadata":{"id":"nJWcDaPpcKCB","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"109c780d-03a7-4dfe-aa75-30aa619e05e5"},"cell_type":"code","source":["model(x).sigmoid()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.9986],\n","        [ 0.9110],\n","        [ 0.9110],\n","        [ 0.1253]])"]},"metadata":{"tags":[]},"execution_count":28}]},{"metadata":{"id":"scIE8zZWdhLs"},"cell_type":"markdown","source":["### Multi-layer neural networks using an optimizer"]},{"metadata":{"id":"J1BrWIxkcMfI"},"cell_type":"code","source":["import torch\n","\n","dtype = torch.float\n","\n","# Training data for XOR.\n","x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n","y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n","                                        \n","# Define a neural network using high-level modules.\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n","    torch.nn.Sigmoid(),                 # Sigmoid function\n","    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",")\n","\n","# Binary corss-entropy loss after sigmoid function.\n","loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n","\n","# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n","\n","for t in range(1000):\n","    y_pred = model(x)           # Make predictions.\n","    loss = loss_fn(y_pred, y)   # Compute the loss.\n","    #print(t, loss.item())\n","    \n","    optimizer.zero_grad()       # Zero-clear gradients.\n","    loss.backward()             # Compute the gradients.\n","    optimizer.step()            # Update the parameters using the gradients."],"execution_count":null,"outputs":[]},{"metadata":{"id":"mp0sNnxhducs","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"eb32ebb1-c216-4858-ad49-405349d6e4e3"},"cell_type":"code","source":["model.state_dict()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('0.weight', tensor([[-5.8059, -5.8059],\n","                      [ 7.3491,  7.3492]])),\n","             ('0.bias', tensor([ 8.6991, -3.3325])),\n","             ('2.weight', tensor([[ 9.7696,  9.6699]])),\n","             ('2.bias', tensor([-14.2857]))])"]},"metadata":{"tags":[]},"execution_count":30}]},{"metadata":{"id":"AYgEteqydwt2","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"91fab3bd-da9a-4e28-d2d1-d5244ca4eadb"},"cell_type":"code","source":["model(x).sigmoid()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0150],\n","        [ 0.9887],\n","        [ 0.9887],\n","        [ 0.0161]])"]},"metadata":{"tags":[]},"execution_count":31}]},{"metadata":{"id":"7x7Ed3SBGjPx"},"cell_type":"markdown","source":["### Single-layer neural network with a customizable NN class."]},{"metadata":{"id":"klKcvlpVkpk9"},"cell_type":"code","source":["import torch\n","\n","dtype = torch.float\n","\n","# Training data for NAND.\n","x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n","y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n","                                        \n","# Define a neural network model.\n","class SingleLayerNN(torch.nn.Module):\n","    def __init__(self, d_in, d_out):\n","        super(SingleLayerNN, self).__init__()\n","        self.linear1 = torch.nn.Linear(d_in, d_out, bias=True)\n","\n","    def forward(self, x):\n","        return self.linear1(x)\n","\n","model = SingleLayerNN(2, 1)\n","\n","# Binary corss-entropy loss after sigmoid function.\n","loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n","\n","# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n","\n","for t in range(100):\n","    y_pred = model(x)           # Make predictions.\n","    loss = loss_fn(y_pred, y)   # Compute the loss.\n","    #print(t, loss.item())\n","    \n","    optimizer.zero_grad()       # Zero-clear gradients.\n","    loss.backward()             # Compute the gradients.\n","    optimizer.step()            # Update the parameters using the gradients."],"execution_count":null,"outputs":[]},{"metadata":{"id":"ExxVfDnB5vPW","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"646b257f-8f9b-4a7b-b5e4-398135dce060"},"cell_type":"code","source":["model.state_dict()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('linear1.weight', tensor([[-4.3039, -4.3036]])),\n","             ('linear1.bias', tensor([ 6.6467]))])"]},"metadata":{"tags":[]},"execution_count":33}]},{"metadata":{"id":"xKczD7tZ518W","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"b6892294-be8e-4148-deae-98ce2544cc3d"},"cell_type":"code","source":["model(x).sigmoid()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.9987],\n","        [ 0.9124],\n","        [ 0.9124],\n","        [ 0.1234]])"]},"metadata":{"tags":[]},"execution_count":34}]},{"metadata":{"id":"jzTPZXUxGNso"},"cell_type":"markdown","source":["### Multi-layer neural network with a customizable NN class.\n","\n"]},{"metadata":{"id":"gSpf1qft53-O"},"cell_type":"code","source":["import torch\n","\n","dtype = torch.float\n","\n","# Training data for XOR.\n","x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n","y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n","                                        \n","# Define a neural network model.\n","class ThreeLayerNN(torch.nn.Module):\n","    def __init__(self, d_in, d_hidden, d_out):\n","        super(ThreeLayerNN, self).__init__()\n","        self.linear1 = torch.nn.Linear(d_in, d_hidden, bias=True)\n","        self.linear2 = torch.nn.Linear(d_hidden, d_out, bias=True)\n","\n","    def forward(self, x):\n","        return self.linear2(self.linear1(x).sigmoid())\n","\n","model = ThreeLayerNN(2, 2, 1)\n","\n","# Binary corss-entropy loss after sigmoid function.\n","loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n","\n","# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n","\n","for t in range(1000):\n","    y_pred = model(x)           # Make predictions.\n","    loss = loss_fn(y_pred, y)   # Compute the loss.\n","    #print(t, loss.item())\n","    \n","    optimizer.zero_grad()       # Zero-clear gradients.\n","    loss.backward()             # Compute the gradients.\n","    optimizer.step()            # Update the parameters using the gradients."],"execution_count":null,"outputs":[]},{"metadata":{"id":"k6LLbCf0684G","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"f7ae4123-1f79-467e-bb46-6635ba0ea628"},"cell_type":"code","source":["model.state_dict()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('linear1.weight', tensor([[-6.7532,  6.5182],\n","                      [ 6.3477, -6.6279]])),\n","             ('linear1.bias', tensor([-3.5757, -3.4740])),\n","             ('linear2.weight', tensor([[ 11.2248,  11.2577]])),\n","             ('linear2.bias', tensor([-5.5168]))])"]},"metadata":{"tags":[]},"execution_count":36}]},{"metadata":{"id":"iPhm3J4R61S2","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"7fcda9bc-ac68-45bd-bbb6-f2193e383451"},"cell_type":"code","source":["model(x).sigmoid()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0076],\n","        [ 0.9942],\n","        [ 0.9942],\n","        [ 0.0066]])"]},"metadata":{"tags":[]},"execution_count":37}]},{"metadata":{"id":"OeOa6mt5Fgw4"},"cell_type":"code","source":[""],"execution_count":null,"outputs":[]}]}